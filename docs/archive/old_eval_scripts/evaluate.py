'''
Automated evaluation script for the Christian music analysis application.

This script compares the analysis generated by the application with a gold standard dataset.
'''

import json
import logging
import os
import re

from app.services.analyzers.router_analyzer import RouterAnalyzer

logging.basicConfig(level=logging.INFO)

def get_gold_standard_files():
    """Get a list of all gold standard files."""
    gold_standard_dir = "gold_standard"
    files = []
    for f in os.listdir(gold_standard_dir):
        if f.endswith(".md") and not f.startswith("dataset_expansion"):
            files.append(os.path.join(gold_standard_dir, f))
    return files

def fetch_lyrics_placeholder(title, artist):
    """
    Placeholder function for lyric fetching.
    In a production system, this would integrate with a lyrics API.
    For now, returns a placeholder to allow testing of the evaluation pipeline.
    """
    return f"[Lyrics for {title} by {artist} would be fetched here]"

def parse_gold_standard_v1(content, file_path):
    """Parse the original gold standard format (with full lyrics and detailed analysis)."""
    try:
        title_match = re.search(r"\*\s*\*\*Title:\*\* (.*)", content)
        artist_match = re.search(r"\*\s*\*\*Artist:\*\* (.*)", content)
        lyrics_match = re.search(r"## Lyrics\n\n(.*?)\n\n## Analysis", content, re.DOTALL)
        score_match = re.search(r"Final Score: (\d+\.?\d*)", content)
        verdict_match = re.search(r"\((Green|Purple|Red)\)", content)

        if all([title_match, artist_match, lyrics_match, score_match, verdict_match]):
            return {
                "title": title_match.group(1).strip(),
                "artist": artist_match.group(1).strip(),
                "lyrics": lyrics_match.group(1).strip(),
                "gold_standard_analysis": {
                    "score": float(score_match.group(1)),
                    "verdict": verdict_match.group(1)
                }
            }
    except (IndexError, ValueError) as e:
        logging.error(f"Error parsing v1 format in {file_path}: {e}")
    return None

def parse_gold_standard_v2(content, file_path):
    """Parse the new gold standard format (with song info and JSON analysis)."""
    try:
        # Extract song information
        title_match = re.search(r"\*\*Title:\*\*\s*(.*)", content)
        artist_match = re.search(r"\*\*Artist:\*\*\s*(.*)", content)
        
        # Extract JSON analysis
        json_match = re.search(r"```json\n(.*?)\n```", content, re.DOTALL)
        
        if all([title_match, artist_match, json_match]):
            try:
                analysis_data = json.loads(json_match.group(1).strip())
                
                # Extract lyrics if present, otherwise use placeholder
                lyrics_match = re.search(r"## Lyrics\n\n(.*?)\n\n## Analysis", content, re.DOTALL)
                lyrics = lyrics_match.group(1).strip() if lyrics_match else None
                
                return {
                    "title": title_match.group(1).strip(),
                    "artist": artist_match.group(1).strip(),
                    "lyrics": lyrics,
                    "gold_standard_analysis": {
                        "score": analysis_data.get("score", 0),
                        "verdict": analysis_data.get("verdict", "Unknown")
                    }
                }
            except json.JSONDecodeError as e:
                logging.error(f"Error parsing JSON in {file_path}: {e}")
    except (IndexError, ValueError) as e:
        logging.error(f"Error parsing v2 format in {file_path}: {e}")
    return None

def parse_gold_standard(file_path):
    """Parse a gold standard file to extract the song information, lyrics, and analysis."""
    logging.info(f"Parsing file: {file_path}")
    
    with open(file_path, "r") as f:
        content = f.read()

    # Try v2 format first (new format with JSON)
    result = parse_gold_standard_v2(content, file_path)
    if result:
        return result
    
    # Fall back to v1 format (original format)
    result = parse_gold_standard_v1(content, file_path)
    if result:
        return result
    
    logging.warning(f"Skipping file (unable to parse): {file_path}")
    return None

def calculate_evaluation_metrics(gold_standard_files_data, generated_analyses):
    """Calculate evaluation metrics comparing gold standard to generated analyses."""
    if not gold_standard_files_data or not generated_analyses:
        return {}
    
    total_songs = len(gold_standard_files_data)
    score_differences = []
    verdict_matches = 0
    
    for i, (gold_data, generated) in enumerate(zip(gold_standard_files_data, generated_analyses)):
        if gold_data and generated:
            # Score difference
            gold_score = gold_data['gold_standard_analysis']['score']
            gen_score = generated.get('score', 0)
            score_diff = abs(gold_score - gen_score)
            score_differences.append(score_diff)
            
            # Verdict match
            gold_verdict = gold_data['gold_standard_analysis']['verdict']
            gen_verdict = generated.get('verdict', 'Unknown')
            if gold_verdict == gen_verdict:
                verdict_matches += 1
    
    metrics = {
        'total_songs': total_songs,
        'avg_score_difference': sum(score_differences) / len(score_differences) if score_differences else 0,
        'verdict_accuracy': verdict_matches / total_songs if total_songs > 0 else 0,
        'max_score_difference': max(score_differences) if score_differences else 0,
        'min_score_difference': min(score_differences) if score_differences else 0
    }
    
    return metrics

def main():
    """Main function to run the evaluation."""
    analyzer = RouterAnalyzer()
    gold_standard_files = get_gold_standard_files()
    
    gold_standard_data = []
    generated_analyses = []

    print("=" * 60)
    print("CHRISTIAN MUSIC ANALYSIS EVALUATION")
    print("=" * 60)
    print()

    for file_path in gold_standard_files:
        song_data = parse_gold_standard(file_path)
        if song_data:
            print(f"Evaluating: {song_data['title']} by {song_data['artist']}")
            
            # Use existing lyrics or fetch placeholder
            lyrics = song_data['lyrics']
            if not lyrics:
                lyrics = fetch_lyrics_placeholder(song_data['title'], song_data['artist'])
                print("  Note: Using placeholder lyrics for analysis")

            generated_analysis = analyzer.analyze_song(song_data['title'], song_data['artist'], lyrics)

            print(f"  Gold Standard: Score {song_data['gold_standard_analysis']['score']}, Verdict: {song_data['gold_standard_analysis']['verdict']}")
            print(f"  Generated:     Score {generated_analysis.get('score', 'N/A')}, Verdict: {generated_analysis.get('verdict', 'N/A')}")
            
            # Store for metrics calculation
            gold_standard_data.append(song_data)
            generated_analyses.append(generated_analysis)
            
            print("-" * 40)
    
    # Calculate and display metrics
    if gold_standard_data and generated_analyses:
        print()
        print("EVALUATION METRICS")
        print("=" * 30)
        metrics = calculate_evaluation_metrics(gold_standard_data, generated_analyses)
        print(f"Total Songs Evaluated: {metrics['total_songs']}")
        print(f"Average Score Difference: {metrics['avg_score_difference']:.2f}")
        print(f"Verdict Accuracy: {metrics['verdict_accuracy']:.1%}")
        print(f"Score Difference Range: {metrics['min_score_difference']:.2f} - {metrics['max_score_difference']:.2f}")
        print()
        
        # Performance assessment
        if metrics['verdict_accuracy'] >= 0.8 and metrics['avg_score_difference'] <= 1.0:
            print("✅ EXCELLENT: Analysis system is performing very well!")
        elif metrics['verdict_accuracy'] >= 0.6 and metrics['avg_score_difference'] <= 2.0:
            print("✅ GOOD: Analysis system is performing adequately.")
        elif metrics['verdict_accuracy'] >= 0.4 and metrics['avg_score_difference'] <= 3.0:
            print("⚠️  FAIR: Analysis system needs improvement.")
        else:
            print("❌ POOR: Analysis system requires significant improvement.")

if __name__ == "__main__":
    main()
