---
trigger: manual
---

## Implementation Guidelines for AI Agent

These guidelines are intended to assist an AI agent like Cursor AI in building the Christian music curation application based on the simplified Flask architecture. The focus is on clarity, modularity, and step-by-step implementation. **This revision strongly recommends using PostgreSQL from the start (via Docker Compose locally) to ensure environment parity and avoid database migration issues.** It also incorporates permanent storage for analysis and Bible verse linking in the MVP.

### 1. Project Setup

*   **Structure:** Use a standard Flask project structure:
    ```
    /christian-music-app
    |-- app/
    |   |-- __init__.py       # Application factory
    |   |-- routes.py         # Flask routes
    |   |-- models.py         # Database models (using SQLAlchemy)
    |   |-- spotify.py        # Spotify API interaction logic (using Spotipy)
    |   |-- analysis.py       # Lyric analysis logic (Transformers, Theme Extraction, Bible Linking)
    |   |-- utils.py          # Helper functions (e.g., Bible verse mapping)
    |   |-- static/           # CSS, JavaScript files
    |   |-- templates/        # Jinja2 HTML templates
    |       |-- base.html
    |       |-- index.html
    |       |-- dashboard.html
    |       |-- playlist_detail.html
    |       |-- login.html
    |-- venv/                 # Virtual environment (Optional if using Docker exclusively)
    |-- requirements.txt      # Python dependencies
    |-- config.py             # Configuration settings
    |-- run.py                # Script to run the Flask development server (mainly for non-Docker use)
    |-- docker-compose.yml    # **Primary method for local development**
    |-- Dockerfile            # For containerization
    ```
*   **Environment:**
    *   **Recommended:** Use Docker Desktop and Docker Compose. Run `docker-compose up` to start the application and database locally.
    *   *Alternative (without Docker):* Create a virtual environment (`python3 -m venv venv`), activate it (`source venv/bin/activate`), install dependencies (`pip install -r requirements.txt`), and ensure a local PostgreSQL server is running and configured.
*   **requirements.txt:** Include necessary libraries:
    ```
    Flask
    spotipy
    python-dotenv
    requests
    lyricsgenius
    transformers
    torch # Or tensorflow, depending on the chosen transformers backend
    SQLAlchemy # Recommended ORM
    Flask-SQLAlchemy # Optional Flask integration for SQLAlchemy
    psycopg2-binary # Or psycopg2, for PostgreSQL connection
    gunicorn # For running the app in Docker/production
    # Add Bible API library if chosen (e.g., python-bible)
    # Add NLP libraries if needed for theme extraction (e.g., nltk, spacy, scikit-learn)
    # Optional: Flask-Login, Flask-Session, Celery, Redis
    ```

### 2. Configuration (`config.py` and `.env`)

*   Use a `config.py` file to manage settings.
*   Store sensitive keys (Spotify Client ID/Secret, Genius Token, Bible API Key, DB Credentials) in a `.env` file and load them using `python-dotenv`.
*   **Configure the database URI using `DATABASE_URL` environment variable for PostgreSQL.**
*   **Example `config.py`:**
    ```python
    import os
    import sys
    from dotenv import load_dotenv

    basedir = os.path.abspath(os.path.dirname(__file__))
    load_dotenv(os.path.join(basedir, ".env"))

    class Config:
        SECRET_KEY = os.environ.get("SECRET_KEY") or "you-will-never-guess"
        
        # Database Configuration (PostgreSQL REQUIRED)
        DATABASE_URL = os.environ.get("DATABASE_URL")
        if not DATABASE_URL:
            sys.exit("Error: DATABASE_URL environment variable is not set. Please configure it in your .env file for PostgreSQL.")
        SQLALCHEMY_DATABASE_URI = DATABASE_URL

        SQLALCHEMY_TRACK_MODIFICATIONS = False
        SPOTIPY_CLIENT_ID = os.environ.get("SPOTIPY_CLIENT_ID")
        SPOTIPY_CLIENT_SECRET = os.environ.get("SPOTIPY_CLIENT_SECRET")
        SPOTIPY_REDIRECT_URI = os.environ.get("SPOTIPY_REDIRECT_URI")
        GENIUS_ACCESS_TOKEN = os.environ.get("GENIUS_ACCESS_TOKEN")
        BIBLE_API_KEY = os.environ.get("BIBLE_API_KEY")
        # Add other config variables as needed
    ```
*   **Example `.env` (for local Docker Compose PostgreSQL):**
    ```
    SECRET_KEY=your_flask_secret_key
    SPOTIPY_CLIENT_ID=your_spotify_client_id
    SPOTIPY_CLIENT_SECRET=your_spotify_client_secret
    SPOTIPY_REDIRECT_URI=http://localhost:5000/callback
    GENIUS_ACCESS_TOKEN=your_genius_access_token
    BIBLE_API_KEY=your_bible_api_key
    # Points to the PostgreSQL service defined in docker-compose.yml
    DATABASE_URL=postgresql://user:password@db:5432/mydatabase 
    FLASK_ENV=development # For Flask development server mode in Docker
    ```
*   **Example `.env` (for production PostgreSQL):**
    ```
    SECRET_KEY=your_strong_production_secret_key
    SPOTIPY_CLIENT_ID=your_spotify_client_id
    SPOTIPY_CLIENT_SECRET=your_spotify_client_secret
    SPOTIPY_REDIRECT_URI=https://yourdomain.com/callback
    GENIUS_ACCESS_TOKEN=your_genius_access_token
    BIBLE_API_KEY=your_bible_api_key
    DATABASE_URL=postgresql://prod_user:prod_password@prod_host:port/prod_dbname
    FLASK_ENV=production
    ```

### 3. Implementation Flow (Step-by-Step)

1.  **Docker Setup (`Dockerfile`, `docker-compose.yml`):** Define the container for the Flask app and the PostgreSQL service.
2.  **Basic Flask App (`app/__init__.py`, `run.py`):** Set up the factory pattern, load config.
3.  **Database Models (`app/models.py`):** Define tables using **SQLAlchemy** for Users, Songs, Playlists, Whitelist.
4.  **Database Migrations (Recommended):** Use `Flask-Migrate` (which uses Alembic) to manage database schema changes. Create initial migration: `flask db init`, `flask db migrate -m "Initial migration"`, `flask db upgrade` (run these commands inside the Docker container or local venv).
5.  **Spotify Authentication (`app/spotify.py`, `app/routes.py`):** Implement OAuth flow.
6.  **Dashboard (`app/routes.py`, `app/spotify.py`, `app/templates/dashboard.html`):** Fetch and display playlists.
7.  **Analysis Logic (`app/analysis.py`):** Implement analysis, theme extraction, Bible linking. Store results permanently in PostgreSQL.
8.  **Playlist Detail View (`app/routes.py`, `app/spotify.py`, `app/analysis.py`, `app/templates/playlist_detail.html`):** Display song details, scores, themes, verses.
9.  **Management Actions (`app/routes.py`, `app/spotify.py`, `app/models.py`):** Implement song removal/whitelisting.
10. **UI Templates (`app/templates/`):** Create Jinja2 templates.

### 4. Modularity and Clarity

*   Keep concerns separated as before.
*   Use clear functions, type hints, and comments.
*   Implement robust error handling and logging.

### 5. Testing

*   Focus on testing critical functions.
*   Testing should primarily target the PostgreSQL database setup used in development and production.

### 6. Deployment Path (Local Docker -> Web)

*   **Local Development (Recommended):**
    *   Install Docker Desktop.
    *   Configure your local `.env` file with `DATABASE_URL=postgresql://user:password@db:5432/mydatabase`.
    *   Run `docker-compose up --build` in the project root.
    *   Access the application at `http://localhost:5000`.
    *   This provides an easy local setup that mirrors the production database environment.

*   **Web Deployment (Scaling Path):**
    *   **Database:** Use a managed PostgreSQL service (e.g., AWS RDS, Google Cloud SQL, Heroku Postgres, Neon, Render PostgreSQL).
    *   **Configuration:** Set production environment variables (including `DATABASE_URL` pointing to the managed PostgreSQL instance, `FLASK_ENV=production`, `SECRET_KEY`, etc.) in your deployment platform.
    *   **Application Deployment:**
        *   **Docker (Recommended):** Build the Docker image and deploy it to your chosen container platform (Render, Fly.io, AWS ECS, Google Cloud Run, etc.). The `Dockerfile` provided should work with minimal changes.
        *   **PaaS (Alternative):** Platforms like Heroku or Render can often build and deploy directly from your Git repository if it includes `requirements.txt` and a `Procfile` (for Heroku: `web: gunicorn run:app`). Ensure the platform uses the correct Python version and installs dependencies.

*   **Dockerfile Example (No changes needed from previous version):**
    ```Dockerfile
    FROM python:3.10-slim
    WORKDIR /app
    COPY requirements.txt requirements.txt
    # Install build dependencies for psycopg2 if needed, then install requirements
    # RUN apt-get update && apt-get install -y --no-install-recommends gcc libpq-dev && rm -rf /var/lib/apt/lists/*
    RUN pip install --no-cache-dir -r requirements.txt
    # Download NLP models/data if needed (e.g., nltk data)
    # RUN python -m nltk.downloader punkt vader_lexicon # Example
    COPY . .
    EXPOSE 5000
    # Use Gunicorn as the WSGI server for production
    CMD ["gunicorn", "-b", ":5000", "run:app"] 
    ```

*   **Docker Compose Example (Local Dev - No changes needed from previous version):**
    ```yaml
    # docker-compose.yml
    version: '3.8'
    services:
      web:
        build: .
        ports:
          - "5000:5000"
        volumes:
          - .:/app # Mount current directory into container for live code changes
        environment:
          - FLASK_ENV=development
          - DATABASE_URL=postgresql://user:password@db:5432/mydatabase
          # Add other necessary environment variables from .env (SPOTIPY_CLIENT_ID, etc.)
          # Consider using env_file: .env instead of listing variables here
        depends_on:
          - db
      db:
        image: postgres:13
        volumes:
          - postgres_data:/var/lib/postgresql/data/
        environment:
          - POSTGRES_USER=user
          - POSTGRES_PASSWORD=password
          - POSTGRES_DB=mydatabase
    volumes:
      postgres_data:
    ```

### 7. Key Considerations for Scaling

*   **Database:** Using PostgreSQL from the start addresses the main scaling bottleneck.
*   **WSGI Server:** `gunicorn` (included in Docker setup) is suitable for production.
*   **Background Tasks:** Implement a task queue (Celery) as needed for long-running analysis tasks.
