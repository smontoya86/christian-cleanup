services:
  web:
    build: .
    # Optimized for ML models: reduced workers, increased timeout and memory
    # Disable max-requests jitter to avoid mid-batch restarts during analysis in dev
    command: gunicorn --bind 0.0.0.0:5000 --workers 1 --timeout 1800 --worker-connections 100 run:app
    ports:
      - "5001:5000"  # Changed external port to 5001 to avoid conflict with macOS Control Center
    volumes:
      - .:/app
      # Add model cache volume for persistent storage
      - model_cache:/root/.cache/huggingface
    environment:
      - OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES  # Required for macOS compatibility
      - FLASK_ENV=${FLASK_ENV}  # Use environment setting from .env
      - ENV=${ENV}  # Use environment setting from .env
      - DEBUG=false  # Production mode
      # Analyzer configuration (Dev -> host Ollama; Prod -> internal vLLM)
      - USE_LLM_ANALYZER=1
      - LLM_API_BASE_URL=http://host.docker.internal:11434/v1
      - LLM_MODEL=llama3.1:8b
      - ENABLE_EMBEDDINGS=1
      - ENABLE_RULES_RAG=1
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - SECRET_KEY=${SECRET_KEY}
      - SPOTIPY_CLIENT_ID=${SPOTIPY_CLIENT_ID}
      - SPOTIPY_CLIENT_SECRET=${SPOTIPY_CLIENT_SECRET}
      - SPOTIPY_REDIRECT_URI=${SPOTIPY_REDIRECT_URI}
      - SPOTIFY_CLIENT_ID=${SPOTIFY_CLIENT_ID}
      - SPOTIFY_CLIENT_SECRET=${SPOTIFY_CLIENT_SECRET}
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - LYRICSGENIUS_API_KEY=${LYRICSGENIUS_API_KEY}
      # Google Analytics
      - GA4_MEASUREMENT_ID=${GA4_MEASUREMENT_ID}
      - GA4_API_SECRET=${GA4_API_SECRET}
      - GA4_DEBUG_MODE=${GA4_DEBUG_MODE}
      # ML optimization settings
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - TOKENIZERS_PARALLELISM=false
      # Parallel backfill configuration
      - BACKFILL_WORKERS=3
      - BACKFILL_BATCH_SIZE=25
    depends_on:
      # In dev with host Ollama, web does not depend on internal llm
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/health"]
      interval: 60s    # Increased interval due to model load time
      timeout: 30s     # Increased timeout
      retries: 3
      start_period: 120s  # Longer startup for model loading
    deploy:
      resources:
        limits:
          memory: 8G      # INCREASED: 4x previous for ML models (was 2G)
          cpus: '2.0'     # INCREASED: 2x previous for ML processing (was 1.0)
        reservations:
          memory: 4G      # INCREASED: 4x previous for ML models (was 1G)
          cpus: '1.0'     # INCREASED: 2x previous for ML processing (was 0.5)
    networks:
      - app-network

  # REMOVED: Worker containers no longer needed
  # Queue system removed - all analysis performed directly in web container using batch processing
  # worker:
  #   build: .
  #   command: bash /app/scripts/docker-entrypoint-worker.sh
  #   depends_on:
  #     - db
  #     - redis
  #   [... configuration removed ...]

  db:
    image: postgres:14-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "127.0.0.1:6380:6379"  # Use port 6380 to avoid conflict with local Redis
    volumes:
      - redis_data:/data
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru --save 60 1000
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G  # Increased for 6 workers
          cpus: '0.5'  # Increased for higher load
        reservations:
          memory: 512M
          cpus: '0.25'
    networks:
      - app-network

  llm:
    image: vllm/vllm-openai:latest
    command: --model meta-llama/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0 --port 8000 --download-dir /data/models --gpu-memory-utilization 0.9
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - model_cache:/data/models
    expose:
      - "8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - app-network

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "127.0.0.1:9090:9090"  # Only expose to localhost for security
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    networks:
      - app-network
    depends_on:
      - web

  grafana:
    image: grafana/grafana:latest
    ports:
      - "127.0.0.1:3000:3000"  # Only expose to localhost for security
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    networks:
      - app-network
    depends_on:
      - prometheus

volumes:
  postgres_data:
  model_cache:  # Persistent storage for HuggingFace model files
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  app-network:
    driver: bridge
